{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import data_generator\n",
    "from args import read_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(A_n=28646, P_n=21044, V_n=18, batch_s=360, checkpoint='', cuda=0, data_path='../data/custom_data_simple/', embed_d=26, in_f_d=128, lr=0.0001, mini_batch_s=360, model_path='../model_save/', random_seed=10, save_model_freq=10, train_iter_n=500, train_test_label=0, walk_L=30, walk_n=10, window=5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = read_args()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading relation files a_a_list.txt\n",
      "Reading relation files a_b_list.txt\n",
      "Reading relation files a_c_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_d_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_e_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_f_list.txt\n",
      "Reading relation files a_g_list.txt\n",
      "\tProcessed 4999 lines\n",
      "Reading relation files a_h_list.txt\n",
      "Reading relation files b_a_list.txt\n",
      "Reading relation files b_b_list.txt\n",
      "Reading relation files b_c_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "\tProcessed 14999 lines\n",
      "\tProcessed 19999 lines\n",
      "\tProcessed 24999 lines\n",
      "Reading relation files b_d_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "\tProcessed 14999 lines\n",
      "Reading relation files b_e_list.txt\n",
      "\tProcessed 4999 lines\n",
      "\tProcessed 9999 lines\n",
      "Reading relation files b_h_list.txt\n",
      "Reading Node Edge Embedding file ../data/custom_data_simple/incoming_edge_embedding.csv\n",
      "Creating Neighbour Edge Embeddings\n"
     ]
    }
   ],
   "source": [
    "input_data = data_generator.input_data(args=args)\n",
    "\n",
    "# import json\n",
    "# with open('../data/custom_data/node_mapping.json') as fin:\n",
    "#     nodem = json.loads(fin.read())\n",
    "\n",
    "# tnodem = {v:k for k,v in nodem.items()} \n",
    "\n",
    "# tnodem['a2']\n",
    "\n",
    "# # nodem['2']\n",
    "\n",
    "# for i in range(len(input_data.a_neigh_list_train)):\n",
    "#     print(f'{i}: {input_data.a_neigh_list_train[i][6]}')\n",
    "\n",
    "# import torch\n",
    "\n",
    "# x = torch.randn(1, 3)\n",
    "# x\n",
    "\n",
    "# x.expand(32,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_edge_embeddings = input_data.node_edge_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_edge_embeddings.shape\n",
    "# n_nodes = 5044482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_incoming_node_embedding = {}\n",
    "# incoming_node_embedding_size = node_edge_embeddings.shape[1] - 2\n",
    "\n",
    "# for row in node_edge_embeddings:\n",
    "#     gid = int(row[0])\n",
    "#     dst_id = int(row[1])\n",
    "    \n",
    "#     if gid not in graph_incoming_node_embedding.keys():\n",
    "#         graph_incoming_node_embedding[gid] = np.zeros((n_dst_node, incoming_node_embedding_size))\n",
    "\n",
    "#     graph_incoming_node_embedding[gid][dst_id] += row[2:]\n",
    "# graph_incoming_node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data.tmp['a_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_data.a_d_edge_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data.incoming_edge_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 3., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 12*26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 1, 12*26).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [3., 3.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [2., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [2., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.a_a_edge_embed[[0,2]].reshape(2, 12*26).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "            input_data.a_a_edge_embed,\n",
    "            input_data.a_b_edge_embed,\n",
    "            input_data.a_c_edge_embed,\n",
    "            input_data.a_d_edge_embed,\n",
    "            input_data.a_e_edge_embed,\n",
    "            input_data.a_f_edge_embed,\n",
    "            input_data.a_g_edge_embed,\n",
    "            input_data.a_h_edge_embed,\n",
    "            input_data.b_a_edge_embed,\n",
    "            input_data.b_b_edge_embed,\n",
    "            input_data.b_c_edge_embed,\n",
    "            input_data.b_d_edge_embed,\n",
    "            input_data.b_e_edge_embed,\n",
    "            input_data.b_h_edge_embed\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fl in enumerate(feature_list):\n",
    "            feature_list[i] = torch.from_numpy(np.array(feature_list[i])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 26])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_id_list = input_data.train_graph_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_train_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tools.HetAgg(args, feature_list, graph_train_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HetAgg(\n",
       "  (fc_a_a_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_b_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_c_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_d_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_e_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_f_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_g_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_a_h_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_a_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_b_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_c_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_d_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_e_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_b_h_agg): Linear(in_features=312, out_features=26, bias=True)\n",
       "  (fc_het_neigh_agg): Linear(in_features=364, out_features=26, bias=True)\n",
       "  (act): LeakyReLU(negative_slope=0.01)\n",
       "  (bn1): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optim = optim.Adam(parameters, lr=args.lr, weight_decay=0)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7755, -0.1655,  0.3050, -0.2328, -0.1327,  0.4544,  0.3475, -0.1647,\n",
      "          0.5719, -0.4296, -0.1055, -0.3185,  0.2556,  0.6084,  0.0895,  0.3053,\n",
      "          0.0995,  0.0056,  0.3828,  0.0666, -0.1630,  0.9905,  0.2334,  0.4522,\n",
      "         -0.0691,  0.2268],\n",
      "        [ 0.7755, -0.1655,  0.3050, -0.2328, -0.1327,  0.4544,  0.3475, -0.1647,\n",
      "          0.5719, -0.4296, -0.1055, -0.3185,  0.2556,  0.6084,  0.0895,  0.3053,\n",
      "          0.0995,  0.0056,  0.3828,  0.0666, -0.1630,  0.9905,  0.2334,  0.4522,\n",
      "         -0.0691,  0.2268]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ 1.0860, -2.8669, -0.6058,  2.2787, -0.3409,  2.7368, -0.6112, -0.8029,\n",
      "         -2.5401,  1.5448, -1.0041,  5.6488,  5.3162,  2.3395,  0.7781, -4.3400,\n",
      "          0.2283,  2.8028, -4.9350,  3.5080,  3.9963,  1.7447,  0.8219,  2.6859,\n",
      "         -5.3805,  2.1236],\n",
      "        [ 1.4421, -3.2238, -0.8721,  2.2471, -0.4530,  2.8013, -0.8891, -0.5240,\n",
      "         -2.6681,  1.6652, -0.9277,  6.2108,  5.4653,  2.7318,  1.1377, -4.7828,\n",
      "          0.3707,  2.8797, -5.2576,  3.7805,  4.3029,  1.8868,  0.6827,  2.9319,\n",
      "         -5.9629,  2.3648]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ -196.4394, -2395.2725,  1756.4539,  -272.1983,  -701.3581,  -847.1774,\n",
      "          1049.0825,  -723.5764, -2801.0376,   698.4690,   642.5774,    32.2348,\n",
      "          1017.4128,   467.7333,  -708.9589, -2153.2290,  -119.9325,  1764.2520,\n",
      "           -53.6141,   455.6201, -1877.4810,   819.4830, -3100.8628, -2750.4504,\n",
      "           837.4230,    34.8163],\n",
      "        [ -519.5780, -1212.4004,  1779.5372,  -552.2546,  -667.1605,  -824.8362,\n",
      "           601.6370,  -986.9377, -1594.8540,  1041.9469,   620.7399,   252.5500,\n",
      "          1266.1046,   174.4754,  -466.9238, -2082.9121,  -139.7561,   866.1470,\n",
      "           994.3563,  -372.2407,  -968.0146,   444.6554, -2812.7573, -2201.0239,\n",
      "            36.0539,  -347.4036]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ -50.4481, -255.0076, -131.2632,  -12.9950, -106.6382,  568.1293,\n",
      "         -257.1282, -250.9827, -396.0636,  325.0849,  370.7330, -263.7504,\n",
      "          -18.1446, -769.5282,  202.6434, -106.1747, -298.9995, -371.6874,\n",
      "         -247.5255,  439.6294, -243.1707, -453.4370,  333.4485,  -90.7121,\n",
      "          301.9538,  630.3304],\n",
      "        [ -51.3427, -259.5313, -133.6002,  -13.2414, -108.5289,  578.2120,\n",
      "         -261.7003, -255.4417, -403.0846,  330.8538,  377.3072, -268.4339,\n",
      "          -18.4608, -783.1819,  206.2339, -108.0606, -304.3078, -378.2773,\n",
      "         -251.9174,  447.4318, -247.4837, -461.4873,  339.3653,  -92.3159,\n",
      "          307.3106,  641.5151]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ -757.2089,  -955.9847,  -453.4601,   981.3018,  -406.4697,  -801.9130,\n",
      "          -246.7422,   404.9558,  -913.0601,  -225.0110,    84.8917,  -552.0183,\n",
      "           598.4662,  -781.0647,   699.7889,  -306.2778,   443.6167, -1756.3717,\n",
      "          1364.4371, -2240.9404,   435.6779,   784.4111,  -588.0092,   265.3227,\n",
      "           473.8547,  -917.5201],\n",
      "        [ -469.8332,  -418.2258,  -469.6463,   661.8612,  -724.2297,  -517.3489,\n",
      "          -269.2213,   654.7013,  -653.3127,  -317.7747,   -65.1717,  -208.5765,\n",
      "           402.8426,  -734.1791,   798.0437,  -403.4560,   236.6680,  -828.8408,\n",
      "          1088.4537, -1488.5153,  -120.8098,   458.8626,  -646.6646,   193.2655,\n",
      "           282.7348,  -652.4022]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0964, -0.0672,  0.5084,  0.2242, -0.7198,  0.5077,  0.2853,  0.1069,\n",
      "         -0.4135,  0.1778, -0.4466,  0.0878,  0.3052,  0.5170,  0.3456, -0.0532,\n",
      "         -0.4737, -0.0998,  0.0367,  0.5259,  0.0353, -0.1030,  0.1897,  0.7460,\n",
      "          0.0422, -0.3337],\n",
      "        [-0.0964, -0.0672,  0.5084,  0.2242, -0.7198,  0.5077,  0.2853,  0.1069,\n",
      "         -0.4135,  0.1778, -0.4466,  0.0878,  0.3052,  0.5170,  0.3456, -0.0532,\n",
      "         -0.4737, -0.0998,  0.0367,  0.5259,  0.0353, -0.1030,  0.1897,  0.7460,\n",
      "          0.0422, -0.3337]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-4.1647, -6.7407, -2.4694,  1.8680, -4.7138, -5.0791, 13.2164, -7.1065,\n",
      "         -3.8468,  1.5487, -2.3407,  2.1837,  9.3313,  5.6971,  3.3055, -0.1985,\n",
      "          6.5176,  0.6992, -5.3327,  0.7579, -2.1110, -0.2461, -2.0379,  9.0024,\n",
      "         -3.5947, -1.3487],\n",
      "        [-4.4514, -7.0542, -2.6098,  2.0847, -5.0402, -5.3474, 13.9892, -7.4124,\n",
      "         -4.0377,  1.6518, -2.5150,  2.2976,  9.9451,  6.0256,  3.5156, -0.0442,\n",
      "          7.1576,  0.4777, -5.7099,  0.6947, -2.1899, -0.2538, -2.0742,  9.8753,\n",
      "         -3.8561, -1.3663]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.9103,  0.3605, -0.4958,  0.0639, -2.1490,  2.1088,  0.1434, -0.4327,\n",
      "         -2.3666,  1.7598,  1.5010, -0.5960,  2.4069, -0.0416,  1.4356,  1.2727,\n",
      "          0.2376, -1.0571, -0.6222, -0.1794,  3.3477, -1.0100,  0.7657,  0.2983,\n",
      "         -1.6161,  0.4325],\n",
      "        [-0.9103,  0.3605, -0.4958,  0.0639, -2.1490,  2.1088,  0.1434, -0.4327,\n",
      "         -2.3666,  1.7598,  1.5010, -0.5960,  2.4069, -0.0416,  1.4356,  1.2727,\n",
      "          0.2376, -1.0571, -0.6222, -0.1794,  3.3477, -1.0100,  0.7657,  0.2983,\n",
      "         -1.6161,  0.4325]], grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0025, -0.0342,  0.0711,  0.0027, -0.0352, -0.1872,  0.2131, -0.1011,\n",
      "          0.2463,  0.3204,  0.1694,  0.1200,  0.0607,  0.2439, -0.1734, -0.0822,\n",
      "          0.1639,  0.2064, -0.0613,  0.0799,  0.2154, -0.2672,  0.1164, -0.0227,\n",
      "          0.0466,  0.0864],\n",
      "        [-0.0025, -0.0342,  0.0711,  0.0027, -0.0352, -0.1872,  0.2131, -0.1011,\n",
      "          0.2463,  0.3204,  0.1694,  0.1200,  0.0607,  0.2439, -0.1734, -0.0822,\n",
      "          0.1639,  0.2064, -0.0613,  0.0799,  0.2154, -0.2672,  0.1164, -0.0227,\n",
      "          0.0466,  0.0864]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ 0.3605,  0.2389,  0.7843,  0.1856,  0.3820, -0.0328,  0.3189, -1.0537,\n",
      "         -0.3917,  0.0614, -0.6215, -0.2212, -0.3264, -0.4363,  0.4143,  0.4526,\n",
      "          0.9161,  0.0902, -0.7393, -0.8820, -0.1448,  0.5691, -0.0025,  1.1832,\n",
      "         -0.1769, -0.1826],\n",
      "        [ 0.5308,  0.0243,  1.0890,  0.0101,  0.2623, -0.0796,  0.7247, -1.2342,\n",
      "         -0.7639, -0.0263, -0.2982, -0.3366, -0.2662,  0.0678, -0.1364,  0.5327,\n",
      "          0.4925,  0.1939, -0.6317, -1.1235, -0.3516,  0.4206,  0.5895,  1.3403,\n",
      "         -0.1850, -0.6106]], grad_fn=<ViewBackward0>)\n",
      "tensor([[ 1.8964e+03,  6.2707e+02, -9.5229e+02,  2.7547e+02, -1.6380e+03,\n",
      "          1.3253e+03, -3.9358e+02, -6.5532e+02, -3.1534e+02,  2.7057e+02,\n",
      "          1.5115e+03, -4.7319e+02,  4.4617e+02, -8.7485e+02, -3.0340e+02,\n",
      "          1.7007e+03, -2.1915e+03, -7.5150e+01,  9.3362e+02, -1.9008e+02,\n",
      "         -2.0477e+03,  7.0415e+02,  6.1700e+01, -4.6396e+02,  4.7909e+02,\n",
      "          2.6447e-01],\n",
      "        [ 1.7566e+03,  3.4264e+02, -6.1638e+02,  6.1133e+02, -1.5706e+03,\n",
      "          1.6728e+03, -5.4929e+02, -1.2627e+03, -3.6007e+02,  2.5943e+02,\n",
      "          1.6314e+03, -4.8260e+01,  4.8199e+02, -9.4136e+02, -3.7915e+02,\n",
      "          1.2814e+03, -2.2846e+03,  9.8646e+01,  1.0772e+03, -2.7026e+02,\n",
      "         -2.4388e+03,  3.5510e+02,  1.3779e+02,  6.9917e+01,  1.1778e+03,\n",
      "          2.0526e+02]], grad_fn=<ViewBackward0>)\n",
      "tensor([[  96.7011, -121.5343,  677.4363,  131.8786, -262.0435,  342.2159,\n",
      "          -47.5399, -267.3888,  593.8341, -215.7534, -482.1536,  -15.3881,\n",
      "          147.2802,  -55.2836,  112.8614,   14.5714, -143.7124, -521.8977,\n",
      "           81.9821,   17.1194,   47.6567,  286.1064,   96.8489,   28.2070,\n",
      "          280.7501, -264.1826],\n",
      "        [  98.4097, -123.6982,  689.4496,  134.2186, -266.6969,  348.2848,\n",
      "          -48.3875, -272.1302,  604.3663, -219.5836, -490.7055,  -15.6621,\n",
      "          149.8935,  -56.2724,  114.8672,   14.8310, -146.2586, -531.1589,\n",
      "           83.4430,   17.4221,   48.5030,  291.1870,   98.5704,   28.6982,\n",
      "          285.7427, -268.8696]], grad_fn=<ViewBackward0>)\n",
      "tensor([[  618.9658,   199.6770,  -468.0572, -1106.2817,  -191.7376,    61.2048,\n",
      "         -1205.4181, -3633.1968,  -597.7639,   374.6503,  -836.9722,  -592.9149,\n",
      "           703.4666,  -305.9527,  1349.8458,  -704.3167, -1287.5725,  -516.0033,\n",
      "         -1039.6080,  -210.1894,   -32.8205,   910.9023,  -181.4037, -1019.0665,\n",
      "           816.7597,   939.4142],\n",
      "        [  288.3620,   297.2016,  -343.4204,  -460.8347,   -19.2998,   271.9205,\n",
      "          -585.5067, -2187.2466,  -743.6191,   614.1750,  -211.2358,  -485.7254,\n",
      "           343.3279,   -97.8534,   640.7914,  -262.9648,  -826.7498,  -605.1870,\n",
      "            99.6920,    20.7274,   -65.8539,   442.1350,  -525.7628,  -595.5320,\n",
      "           303.6577,   653.6476]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100,  1.0000,  1.0000,  0.9999,  1.0000, -0.0100,  0.9999, -0.0100,\n",
       "          1.0000,  1.0000, -0.0100, -0.0100,  0.9986,  0.9834,  1.0000, -0.0100,\n",
       "         -0.0100, -0.0100,  1.0000, -0.0100, -0.0100,  1.0000,  1.0000,  0.9949,\n",
       "          1.0000, -0.0100],\n",
       "        [ 1.0000, -0.0100, -0.0100, -0.0100, -0.0100,  1.0000, -0.0100,  1.0000,\n",
       "         -0.0100, -0.0100,  1.0000,  0.9998, -0.0100, -0.0098, -0.0100,  1.0000,\n",
       "          1.0000,  1.0000, -0.0100,  0.9983,  1.0000, -0.0100, -0.0100, -0.0099,\n",
       "         -0.0100,  0.9986]], grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "model([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0100,  0.9998,  0.9997,  0.9806,  0.9984, -0.0100,  0.9997, -0.0100,\n",
       "          0.9988, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100,  0.9999,\n",
       "         -0.0100, -0.0100,  0.9998, -0.0100, -0.0100, -0.0100,  0.9990, -0.0100,\n",
       "          0.9999, -0.0100],\n",
       "        [ 0.9998, -0.0100, -0.0100, -0.0098, -0.0100,  0.9952, -0.0100,  0.9997,\n",
       "         -0.0100,  0.9986,  0.9966,  0.9999,  0.9991,  0.9999,  0.9998, -0.0100,\n",
       "          0.9990,  0.9966, -0.0100,  0.9997,  0.9998,  0.9990, -0.0100,  0.9997,\n",
       "         -0.0100,  0.9997]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.edge_content_agg([0,1], 'a_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7091, -0.5976,  1.2280,  0.9533,  1.4142,  0.1888,  1.3949, -1.3406,\n",
       "         -0.7011, -0.6019, -0.7125, -0.6974,  0.0876, -0.7029, -0.7118, -0.6222,\n",
       "         -0.7112,  0.1086, -0.6999, -0.6423, -0.6474, -0.5689,  1.4141, -0.6973,\n",
       "         -0.5421, -0.7000],\n",
       "        [ 1.4142, -0.8112, -1.2213,  0.4279, -0.7143,  1.1193, -0.8992,  0.2805,\n",
       "         -0.7130,  1.4092, -0.7017,  1.4141,  1.1786,  1.4142,  1.4142, -0.7887,\n",
       "          1.4142,  1.1668, -0.7143,  1.4123,  1.4126,  1.4057, -0.6971,  1.4141,\n",
       "         -0.8601,  1.4142],\n",
       "        [-0.7051,  1.4088, -0.0067, -1.3812, -0.6998, -1.3081, -0.4957,  1.0601,\n",
       "          1.4142, -0.8073,  1.4142, -0.7168, -1.2661, -0.7113, -0.7023,  1.4109,\n",
       "         -0.7030, -1.2754,  1.4142, -0.7700, -0.7651, -0.8368, -0.7170, -0.7168,\n",
       "          1.4022, -0.7141]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.bn1()\n",
    "model.bn1(model.edge_content_agg([0,1,3], 'a_b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdd_batch_loss(embed_batch): #nu: {0.1, 0.01}\n",
    "    _batch_out = embed_batch\n",
    "    _batch_out_resahpe = _batch_out.view(_batch_out.size()[0] * _batch_out.size()[1], embed_d)\n",
    "    \n",
    "    hypersphere_center = torch.mean(_batch_out_resahpe, 0)\n",
    "    \n",
    "    dist = torch.square(_batch_out_resahpe - hypersphere_center)\n",
    "    \n",
    "    return torch.mean(torch.sum(dist, 1))\n",
    "\n",
    "# svdd_batch_loss(_out, batch_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ...\n",
      "Batch Loss: 998769.1875\n",
      "Batch Loss: 19455156.0\n",
      "iteration 1 ...\n",
      "Batch Loss: 656990.375\n",
      "Batch Loss: 14246745.0\n",
      "iteration 2 ...\n",
      "Batch Loss: 419677.46875\n",
      "Batch Loss: 10036445.0\n",
      "iteration 3 ...\n",
      "Batch Loss: 257699.046875\n",
      "Batch Loss: 6800866.0\n",
      "iteration 4 ...\n",
      "Batch Loss: 151272.515625\n",
      "Batch Loss: 4402917.0\n",
      "iteration 5 ...\n",
      "Batch Loss: 83769.6953125\n",
      "Batch Loss: 2691663.0\n",
      "iteration 6 ...\n",
      "Batch Loss: 44503.828125\n",
      "Batch Loss: 1494286.125\n",
      "iteration 7 ...\n",
      "Batch Loss: 22803.3125\n",
      "Batch Loss: 735565.875\n",
      "iteration 8 ...\n",
      "Batch Loss: 10926.998046875\n",
      "Batch Loss: 327469.9375\n",
      "iteration 9 ...\n",
      "Batch Loss: 4741.4248046875\n",
      "Batch Loss: 147639.328125\n",
      "iteration 10 ...\n",
      "Batch Loss: 2171.076904296875\n",
      "Batch Loss: 74407.3125\n",
      "iteration 11 ...\n",
      "Batch Loss: 1261.9591064453125\n",
      "Batch Loss: 43297.19921875\n",
      "iteration 12 ...\n",
      "Batch Loss: 940.5889282226562\n",
      "Batch Loss: 26772.544921875\n",
      "iteration 13 ...\n",
      "Batch Loss: 806.6353759765625\n",
      "Batch Loss: 16400.189453125\n",
      "iteration 14 ...\n",
      "Batch Loss: 738.5416259765625\n",
      "Batch Loss: 9976.228515625\n",
      "iteration 15 ...\n",
      "Batch Loss: 700.9281005859375\n",
      "Batch Loss: 6447.265625\n",
      "iteration 16 ...\n",
      "Batch Loss: 677.9112548828125\n",
      "Batch Loss: 4751.314453125\n",
      "iteration 17 ...\n",
      "Batch Loss: 670.42919921875\n",
      "Batch Loss: 4031.75\n",
      "iteration 18 ...\n",
      "Batch Loss: 671.4129638671875\n",
      "Batch Loss: 3694.416748046875\n",
      "iteration 19 ...\n",
      "Batch Loss: 673.77099609375\n",
      "Batch Loss: 3593.847900390625\n",
      "iteration 20 ...\n",
      "Batch Loss: 675.9747314453125\n",
      "Batch Loss: 3609.462890625\n",
      "iteration 21 ...\n",
      "Batch Loss: 676.572265625\n",
      "Batch Loss: 3643.2919921875\n",
      "iteration 22 ...\n",
      "Batch Loss: 675.72021484375\n",
      "Batch Loss: 3671.4345703125\n",
      "iteration 23 ...\n",
      "Batch Loss: 673.9902954101562\n",
      "Batch Loss: 3693.689208984375\n",
      "iteration 24 ...\n",
      "Batch Loss: 671.8438720703125\n",
      "Batch Loss: 3709.328369140625\n",
      "iteration 25 ...\n",
      "Batch Loss: 669.6397094726562\n",
      "Batch Loss: 3716.677490234375\n",
      "iteration 26 ...\n",
      "Batch Loss: 667.6743774414062\n",
      "Batch Loss: 3715.890380859375\n",
      "iteration 27 ...\n",
      "Batch Loss: 666.185302734375\n",
      "Batch Loss: 3708.943359375\n",
      "iteration 28 ...\n",
      "Batch Loss: 665.2947998046875\n",
      "Batch Loss: 3698.694091796875\n",
      "iteration 29 ...\n",
      "Batch Loss: 664.9873046875\n",
      "Batch Loss: 3686.6474609375\n",
      "iteration 30 ...\n",
      "Batch Loss: 665.2075805664062\n",
      "Batch Loss: 3673.447509765625\n",
      "iteration 31 ...\n",
      "Batch Loss: 665.3685302734375\n",
      "Batch Loss: 3659.99365234375\n",
      "iteration 32 ...\n",
      "Batch Loss: 665.2614135742188\n",
      "Batch Loss: 3646.066650390625\n",
      "iteration 33 ...\n",
      "Batch Loss: 664.9334106445312\n",
      "Batch Loss: 3631.748779296875\n",
      "iteration 34 ...\n",
      "Batch Loss: 664.4263305664062\n",
      "Batch Loss: 3617.21240234375\n",
      "iteration 35 ...\n",
      "Batch Loss: 663.7718505859375\n",
      "Batch Loss: 3603.27587890625\n",
      "iteration 36 ...\n",
      "Batch Loss: 663.1018676757812\n",
      "Batch Loss: 3588.21044921875\n",
      "iteration 37 ...\n",
      "Batch Loss: 662.41064453125\n",
      "Batch Loss: 3573.84423828125\n",
      "iteration 38 ...\n",
      "Batch Loss: 661.6077880859375\n",
      "Batch Loss: 3560.382568359375\n",
      "iteration 39 ...\n",
      "Batch Loss: 660.8104248046875\n",
      "Batch Loss: 3545.31494140625\n",
      "iteration 40 ...\n",
      "Batch Loss: 660.0126342773438\n",
      "Batch Loss: 3531.109130859375\n",
      "iteration 41 ...\n",
      "Batch Loss: 659.1287231445312\n",
      "Batch Loss: 3516.8857421875\n",
      "iteration 42 ...\n",
      "Batch Loss: 658.1675415039062\n",
      "Batch Loss: 3502.501220703125\n",
      "iteration 43 ...\n",
      "Batch Loss: 657.1376342773438\n",
      "Batch Loss: 3487.982177734375\n",
      "iteration 44 ...\n",
      "Batch Loss: 656.0504760742188\n",
      "Batch Loss: 3473.37353515625\n",
      "iteration 45 ...\n",
      "Batch Loss: 654.916748046875\n",
      "Batch Loss: 3458.71728515625\n",
      "iteration 46 ...\n",
      "Batch Loss: 653.8171997070312\n",
      "Batch Loss: 3444.06494140625\n",
      "iteration 47 ...\n",
      "Batch Loss: 652.7649536132812\n",
      "Batch Loss: 3429.458740234375\n",
      "iteration 48 ...\n",
      "Batch Loss: 651.6957397460938\n",
      "Batch Loss: 3414.925537109375\n",
      "iteration 49 ...\n",
      "Batch Loss: 650.5946655273438\n",
      "Batch Loss: 3400.495849609375\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch_s = args.batch_s\n",
    "mini_batch_s = args.mini_batch_s\n",
    "embed_d = args.embed_d\n",
    "\n",
    "for iter_i in range(args.train_iter_n):\n",
    "    print('iteration ' + str(iter_i) + ' ...')\n",
    "    gid_list = np.array(range(600))\n",
    "    batch_list = gid_list.reshape(int(600 / batch_s), batch_s)\n",
    "\n",
    "    \n",
    "    for batch_n, k in enumerate(batch_list):\n",
    "        _out = torch.zeros(int(len(k) / mini_batch_s), mini_batch_s, embed_d)\n",
    "        mini_batch_list = k.reshape(int(len(k) / mini_batch_s), mini_batch_s)\n",
    "        for mini_n, mini_k in enumerate(mini_batch_list):\n",
    "            _out_temp = model(mini_k)\n",
    "            _out[mini_n] = _out_temp\n",
    "        \n",
    "        batch_loss = tools.svdd_batch_loss(_out)\n",
    "        print(f'Batch Loss: {batch_loss}')\n",
    "        optim.zero_grad()\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3400.495849609375"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100, 26])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([657.5353, 863.0911, 615.0443,   7.6595,  11.6553,  17.7259,  -7.7687,\n",
       "        166.3391,  23.2053, 648.8605, 561.4694, 185.6070, 185.3958, 142.6107,\n",
       "         -6.4988,  -5.6927, 240.8286, 101.9214, 343.2222,  25.8779, 198.3611,\n",
       "         34.5414, 134.2992,  29.5433, 154.0036, 547.3102],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypersphere_center = torch.mean(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d), 0)\n",
    "hypersphere_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 461.9786,  459.6783, -233.7465,  ...,  -33.1201, -159.7627,\n",
       "          367.8847],\n",
       "        [ 295.4781,  132.3753, -248.0656,  ...,  -32.7902, -155.9181,\n",
       "          104.9939],\n",
       "        [ 816.1484, 1347.2153,  857.7902,  ..., 1451.0356,  265.1805,\n",
       "          645.9609],\n",
       "        ...,\n",
       "        [ 169.4581, -119.2835,  154.2783,  ...,  447.9461, -155.6930,\n",
       "         -113.7345],\n",
       "        [-100.5992, -754.9912,  -57.0236,  ...,  -40.5429, -158.7739,\n",
       "           61.5334],\n",
       "        [-283.9245,  122.8851,  116.9337,  ...,  -30.9534,  300.8385,\n",
       "          557.7177]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4155.9966, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mean(torch.sum(torch.sqrt(torch.square(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center)), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1942913.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(torch.square(_out[batch_n].view(len(mini_batch_list) * mini_batch_s, embed_d) - hypersphere_center), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(range(600))\n",
    "np.random.shuffle(a)\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gid_list = np.array(range(600))\n",
    "benign_gid_list = all_gid_list[(all_gid_list < 300) | (all_gid_list > 399)]\n",
    "attack_gid_list = np.array(range(300, 400))\n",
    "\n",
    "# Train/Eval/Test = 0.6/0.2/0.2\n",
    "train_benign_gid_list = np.random.choice(benign_gid_list, 360, replace=False)\n",
    "left_benign_gid_list = benign_gid_list[np.in1d(\n",
    "    benign_gid_list, train_benign_gid_list, invert=True)]\n",
    "\n",
    "eval_benign_gid_list = np.random.choice(left_benign_gid_list, 70, replace=False)\n",
    "test_benign_gid_list = left_benign_gid_list[np.in1d(\n",
    "    left_benign_gid_list, eval_benign_gid_list, invert=True)]\n",
    "\n",
    "train_attack_gid_list = np.random.choice(attack_gid_list, 60, replace=False)\n",
    "left_attack_gid_list = attack_gid_list[np.in1d(\n",
    "    attack_gid_list, train_attack_gid_list, invert=True)]\n",
    "eval_attack_gid_list = np.random.choice(left_attack_gid_list, 20, replace=False)\n",
    "test_attack_gid_list = left_attack_gid_list[np.in1d(\n",
    "    left_attack_gid_list, eval_attack_gid_list, invert=True)]\n",
    "\n",
    "train_gid_list = np.concatenate([train_benign_gid_list, train_attack_gid_list], axis=0) \n",
    "eval_gid_list = np.concatenate([eval_benign_gid_list, eval_attack_gid_list], axis=0)\n",
    "test_gid_list = np.concatenate([test_benign_gid_list, test_attack_gid_list], axis=0)\n",
    "\n",
    "np.random.shuffle(train_gid_list)\n",
    "np.random.shuffle(eval_gid_list)\n",
    "np.random.shuffle(test_gid_list)\n",
    "np.random.shuffle(benign_gid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([395, 529, 552, 542, 462, 534,  40,  34, 179, 247, 480,  89,  45,\n",
       "       157, 114, 283, 550,  69, 402, 543, 476,  78,  26, 585, 199, 285,\n",
       "       267,  98, 341,  99, 230, 223, 404, 459,  42,  79, 464,  82, 165,\n",
       "       405, 184,  83, 259,   0, 557, 590, 394, 372, 272, 121, 486, 347,\n",
       "       517, 444, 554, 428, 384, 222, 271,  46, 280,  24, 148, 200, 207,\n",
       "        28, 562, 407, 409, 185,  54, 235,  13, 507, 385, 566, 158, 526,\n",
       "       286, 248, 256, 252, 155, 153, 110, 437, 133, 328, 494, 460, 545,\n",
       "       128, 573, 275, 596, 126,  65, 522, 589, 523, 423, 571, 583, 258,\n",
       "       310, 152, 196, 365, 598, 216, 375, 575, 503, 338, 490, 530, 445,\n",
       "       449, 333, 564, 304,  30, 364,  75, 474, 454, 380, 177, 466, 357,\n",
       "       171, 173, 297, 498, 332, 197, 346, 563, 371,  38,  80, 447, 298,\n",
       "       130, 142,  22, 509, 262, 349,  70, 591, 181, 147, 548, 439, 212,\n",
       "       456, 481,  81, 389, 241, 254, 546, 540, 138,  64, 426, 484,   8,\n",
       "       443, 170, 136,  14, 452, 166,  55, 475, 293, 581, 537, 593,  39,\n",
       "       226, 261, 279, 265,  37, 343,  47, 502, 576,  88, 264,  90, 113,\n",
       "       299, 206, 122, 103,  61, 535, 401, 353, 467, 538, 313,  92, 578,\n",
       "       109, 270, 129, 263, 547, 143, 376, 131, 183, 559, 154, 117,  29,\n",
       "       239, 145, 202, 144, 160, 422, 137, 499, 400,   7, 116, 276, 383,\n",
       "        68, 309,  25, 187, 374, 198, 210, 302, 242, 436, 479, 245,  31,\n",
       "       151, 415, 268,   2, 442, 378, 565, 392, 373, 553, 274, 296, 295,\n",
       "       427,  52, 528, 558, 211, 169, 350, 290, 465,  41, 180, 527, 100,\n",
       "       312, 524, 201, 555,  50, 132, 525, 337, 521, 186, 536, 382, 354,\n",
       "       441, 472, 320, 398, 506, 266,  48, 191, 424, 311, 217, 416, 366,\n",
       "       435, 470, 269, 463, 358, 345, 533, 225, 246, 478, 519,  17, 277,\n",
       "       250, 316,  32, 403, 205, 431, 330, 119, 489,   6,  20, 228, 300,\n",
       "        94,  10, 106, 244, 497, 339, 303, 584, 448,  36, 434, 257, 204,\n",
       "       570, 446, 240, 496, 123, 367, 305, 215, 387,  16,  84, 146, 572,\n",
       "       359,   9, 319, 231, 397,  93, 568, 323, 421, 511, 251,  63, 161,\n",
       "       120, 233, 582,  85, 111, 417, 488, 255, 561, 418, 471, 580, 482,\n",
       "       419, 175, 214, 515, 510, 485, 334, 493, 194, 308,  49, 176, 420,\n",
       "       278, 429,  21, 124, 495, 504, 118,  53, 294, 115,  11, 249, 208,\n",
       "       450, 455, 317, 412,  19, 178, 156, 500, 284, 579, 192, 386, 406,\n",
       "        77, 190,   3, 501])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([236, 520, 224, 599, 440,  74,   1, 551, 379, 492, 189, 105, 487,\n",
       "       544,  27, 411, 253, 306, 238, 307,  66, 348, 414, 410, 512, 438,\n",
       "       560, 425, 549, 229,  72, 597, 532, 483, 141, 291,  18, 458, 227,\n",
       "       469,  57, 390,  43, 352,   4, 326, 322, 288, 168, 336, 453, 577,\n",
       "       331, 167, 102, 569, 329,  33, 108,  12,  91, 135, 112, 361,  59,\n",
       "       393, 125, 574, 301,  44, 505, 508, 287, 209,  60, 159, 101, 362,\n",
       "       461, 172,  76, 360,  62, 388, 218,  71, 314, 213, 324, 344])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([335,  35,  86, 149, 150, 140, 104, 282, 107, 539, 237, 163, 588,\n",
       "       592, 234, 342, 232, 368, 219, 381, 315, 273, 413, 594, 327, 321,\n",
       "       399,  97,  87, 162, 243, 370, 325, 541, 432, 134, 457, 477, 391,\n",
       "       595,  51, 516, 513, 556, 164,  58,  15, 363,  67, 408, 531, 491,\n",
       "       340, 468, 473, 514, 281, 127, 260, 292, 174, 356, 430, 396, 351,\n",
       "        95, 369, 195,  56, 139,  96, 451, 377, 433, 193,  23, 182, 289,\n",
       "       220, 203, 518, 221,  73, 587,   5, 586, 318, 188, 355, 567])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((eval_gid_list >= 300) & (eval_gid_list < 400), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X = model(eval_gid_list)\n",
    "true_y = np.where((eval_gid_list >= 300) & (eval_gid_list < 400), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(np.array(pred_X.tolist()), true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.07654476,  -3.67806578,  -3.97940493, ...,  -5.61856508,\n",
       "         -6.63739014,  -3.286201  ],\n",
       "       [-25.30461121, -17.8821907 , -19.46343231, ..., -28.46804428,\n",
       "        -23.69834137, -15.92532158],\n",
       "       [ -4.52741814,  -4.9840641 ,  -4.52318239, ...,  -5.44792175,\n",
       "         -7.24286461,  -6.37920046],\n",
       "       ...,\n",
       "       [-26.39011574, -21.42318153, -20.97297478, ..., -29.66191101,\n",
       "        -19.13190269, -20.55114174],\n",
       "       [ -4.7614584 ,  -4.15464735,  -2.90741062, ...,  -5.77147436,\n",
       "         -6.88473701,  -5.26079416],\n",
       "       [-29.96796799, -19.19325066, -22.16016579, ..., -27.75070953,\n",
       "        -31.17169762, -20.11503029]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ece4cc9c4aede854b5983a138a0d4c9cc415fe9dd0477bef89310f7b90319c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
